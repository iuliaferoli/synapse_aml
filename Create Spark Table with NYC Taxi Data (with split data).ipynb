{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\r\n",
        "Get a sample data of nyc yellow taxi from Azure Open Datasets"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import NycTlcYellow\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "\r\n",
        "start_date = parser.parse('2018-05-01')\r\n",
        "end_date = parser.parse('2018-05-07')\r\n",
        "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
        "nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n",
        "nyc_tlc_df.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\r\n",
        "\r\n",
        "sampled_df = nyc_tlc_df.sample(n=10000, random_state=123)\r\n",
        "display(sampled_df.head(5))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and featurize data\r\n",
        "- There are extra dimensions that are not going to be useful in the model. We just take the dimensions that we need and put them into the featurised dataframe. \r\n",
        "- There are also a bunch of outliers in the data so we need to filter them out."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\r\n",
        "import pandas\r\n",
        "\r\n",
        "def get_pickup_time(df):\r\n",
        "    pickupHour = df['pickupHour'];\r\n",
        "    if ((pickupHour >= 7) & (pickupHour <= 10)):\r\n",
        "        return 'AMRush'\r\n",
        "    elif ((pickupHour >= 11) & (pickupHour <= 15)):\r\n",
        "        return 'Afternoon'\r\n",
        "    elif ((pickupHour >= 16) & (pickupHour <= 19)):\r\n",
        "        return 'PMRush'\r\n",
        "    else:\r\n",
        "        return 'Night'\r\n",
        "\r\n",
        "featurized_df = pandas.DataFrame()\r\n",
        "featurized_df['tipped'] = (sampled_df['tipAmount'] > 0).astype('int')\r\n",
        "featurized_df['fareAmount'] = sampled_df['fareAmount'].astype('float32')\r\n",
        "featurized_df['paymentType'] = sampled_df['paymentType'].astype('int')\r\n",
        "featurized_df['passengerCount'] = sampled_df['passengerCount'].astype('int')\r\n",
        "featurized_df['tripDistance'] = sampled_df['tripDistance'].astype('float32')\r\n",
        "featurized_df['pickupHour'] = sampled_df['tpepPickupDateTime'].dt.hour.astype('int')\r\n",
        "featurized_df['tripTimeSecs'] = ((sampled_df['tpepDropoffDateTime'] - sampled_df['tpepPickupDateTime']) / numpy.timedelta64(1, 's')).astype('int')\r\n",
        "\r\n",
        "featurized_df['pickupTimeBin'] = featurized_df.apply(get_pickup_time, axis=1)\r\n",
        "featurized_df = featurized_df.drop(columns='pickupHour')\r\n",
        "\r\n",
        "display(featurized_df.head(5))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = featurized_df[(featurized_df.tipped >= 0) & (featurized_df.tipped <= 1)\\\r\n",
        "    & (featurized_df.fareAmount >= 1) & (featurized_df.fareAmount <= 250)\\\r\n",
        "    & (featurized_df.paymentType >= 1) & (featurized_df.paymentType <= 2)\\\r\n",
        "    & (featurized_df.passengerCount > 0) & (featurized_df.passengerCount < 8)\\\r\n",
        "    & (featurized_df.tripDistance >= 0) & (featurized_df.tripDistance <= 100)\\\r\n",
        "    & (featurized_df.tripTimeSecs >= 30) & (featurized_df.tripTimeSecs <= 7200)]\r\n",
        "\r\n",
        "filtered_df.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into train and test\n",
        "* We will run the AutoML model on the training set, and use the testing set to make predictions on the SQL pool"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(filtered_df, test_size=0.3, random_state=123)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the training data to spark table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame(train_df)\r\n",
        "spark_df.write.mode(\"overwrite\").saveAsTable(\"default.NYC_Taxi_train\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the test data to ADLS gen2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_test_df = spark.createDataFrame(test_df)\r\n",
        "spark_test_df = spark_test_df.repartition(1) # This ensure we'll get a single file during write()\r\n",
        "spark_test_df.write.mode(\"overwrite\").csv(\"/NYCTaxi/testdata\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}